// Mirrors the `jax.lax` module in JAX.
//
// Unlike in JAX, this does not actually underpin `jax.numpy` as a more "core"
// set of operations, as they both build open the same foundations.

const JsArray = globalThis.Array;

import { DType } from "../alu";
import { Array, ArrayLike, fudgeArray, zerosLike } from "../frontend/array";
import * as core from "../frontend/core";
import { bind1, Primitive } from "../frontend/core";
import { moveaxis, vmap } from "../frontend/vmap";
import { Pair } from "../shape";
import { checkAxis, deepEqual, prod, range, rep, zipn } from "../utils";

export * as linalg from "./lax-linalg";
export { scan } from "./lax-scan";
export type { ScanOptions } from "./lax-scan";

/**
 * Dimension numbers for general `dot()` primitive.
 *
 * Contracting dimensions act as a tensor contraction (reduction) along the
 * given axis. They must be the same size in both operands. Batch dimensions
 * are treated as vectorized, leading batch dimensions.
 *
 * The return value has a shape where the first dimensions are shared batch
 * dimensions, followed by `lhs` non-contracting dimensions, followed by
 * `rhs` non-contracting dimensions.
 */
export type DotDimensionNumbers = {
  lhsContractingDims?: number[];
  rhsContractingDims?: number[];
  lhsBatchDims?: number[];
  rhsBatchDims?: number[];
};

/**
 * General dot product/contraction operator.
 *
 * Prefer higher-level functions like `jax.numpy.dot()`, `jax.numpy.matmul()`,
 * `jax.numpy.tensordot(), and `jax.numpy.einsum()` where possible.
 */
export function dot(
  lhs: Array,
  rhs: Array,
  {
    lhsContractingDims: lc = [],
    rhsContractingDims: rc = [],
    lhsBatchDims: lb = [],
    rhsBatchDims: rb = [],
  }: DotDimensionNumbers = {},
): Array {
  // First do input validation, helps with debugging.
  if (lc.length !== rc.length) {
    throw new Error(
      `dot: contracting dims lengths mismatch, got ${JSON.stringify(lc)} and ${JSON.stringify(rc)}`,
    );
  } else if (lb.length !== rb.length) {
    throw new Error(
      `dot: batch dims lengths mismatch, got ${JSON.stringify(lb)} and ${JSON.stringify(rb)}`,
    );
  }
  lc = lc.map((a) => checkAxis(a, lhs.ndim));
  rc = rc.map((a) => checkAxis(a, rhs.ndim));
  lb = lb.map((a) => checkAxis(a, lhs.ndim));
  rb = rb.map((a) => checkAxis(a, rhs.ndim));
  if (lc.some((a) => lb.includes(a))) {
    throw new Error(
      `dot: lhs contracting dims ${JSON.stringify(lc)} ` +
        `overlap with batch dims ${JSON.stringify(lb)}`,
    );
  } else if (rc.some((a) => rb.includes(a))) {
    throw new Error(
      `dot: rhs contracting dims ${JSON.stringify(rc)} ` +
        `overlap with batch dims ${JSON.stringify(rb)}`,
    );
  }

  // Compute "free" dimensions: output shape is [...{lb/rb}, ...lf, ...rf].
  const lf = range(lhs.ndim).filter((a) => !lc.includes(a) && !lb.includes(a));
  const rf = range(rhs.ndim).filter((a) => !rc.includes(a) && !rb.includes(a));
  using lhs2 = lhs.transpose([...lb, ...lf, ...lc]);
  using rhs2 = rhs.transpose([...rb, ...rf, ...rc]);

  if (lc.length === 0) {
    // There is no contraction to perform, just do a product (not `dot`).
    using lhsR = lhs2.reshape([
      ...lb.map((a) => lhs.shape[a]),
      ...lf.map((a) => lhs.shape[a]),
      ...rep(rf.length, 1),
    ]);
    using rhsR = rhs2.reshape([
      ...rb.map((a) => rhs.shape[a]),
      ...rep(lf.length, 1),
      ...rf.map((a) => rhs.shape[a]),
    ]);
    return core.mul(lhsR, rhsR) as Array;
  }

  // Otherwise, we need to do a `dot` contraction.
  const dotShapeX = lc.map((a) => lhs.shape[a]);
  const dotShapeY = rc.map((a) => rhs.shape[a]);
  if (!deepEqual(dotShapeX, dotShapeY)) {
    throw new Error(
      `dot: shapes not aligned along contracting dims:` +
        ` ${JSON.stringify(dotShapeX)} != ${JSON.stringify(dotShapeY)}`,
    );
  }
  using lhsR = lhs2.reshape([
    ...lb.map((a) => lhs.shape[a]),
    ...lf.map((a) => lhs.shape[a]),
    ...rep(rf.length, 1),
    prod(dotShapeX),
  ]);
  using rhsR = rhs2.reshape([
    ...rb.map((a) => rhs.shape[a]),
    ...rep(lf.length, 1),
    ...rf.map((a) => rhs.shape[a]),
    prod(dotShapeY),
  ]);
  return core.dot(lhsR, rhsR) as Array;
}

export type PaddingType = "VALID" | "SAME" | "SAME_LOWER" | Pair[];

function padtypeToPads(
  inShape: number[],
  filterShape: number[],
  strides: number[],
  dilation: number[],
  padding: string,
): [number, number][] {
  const padType = padding.toUpperCase();
  switch (padType) {
    case "VALID":
      return rep<[number, number]>(inShape.length, [0, 0]);
    case "SAME":
    case "SAME_LOWER": {
      const outShape = inShape.map((size, i) => Math.ceil(size / strides[i]));
      const padSizes = zipn(
        outShape,
        strides,
        filterShape,
        dilation,
        inShape,
      ).map(([o, s, k, d, i]) =>
        Math.max(0, (o - 1) * s + 1 + (k - 1) * d - i),
      );
      if (padType === "SAME") {
        return padSizes.map((size) => [size >> 1, size - (size >> 1)]);
      } else {
        return padSizes.map((size) => [size - (size >> 1), size >> 1]);
      }
    }
    default:
      throw new Error(`Unknown padding type: ${padType}`);
  }
}

/**
 * General n-dimensional convolution operator, with optional dilation.
 *
 * The semantics of this operation mimic the `jax.lax.conv_general_dilated`
 * function in JAX, which wraps XLA's general convolution operator.
 *
 * @param lhs - Input tensor; shape `[N, C_in, ...xs]`
 * @param rhs - Convolution kernel; shape `[C_out, C_in / G, ...ks]`
 * @param windowStrides - Strides for each spatial dimension
 * @param padding - Padding for each spatial dimension, or a string
 *   (`"VALID"`, `"SAME"`, or `"SAME_LOWER"`)
 */
export function convGeneralDilated(
  lhs: Array,
  rhs: Array,
  windowStrides: number[],
  padding: PaddingType,
  {
    lhsDilation,
    rhsDilation,
    featureGroupCount = 1,
  }: {
    lhsDilation?: number[];
    rhsDilation?: number[];
    featureGroupCount?: number;
  } = {},
): Array {
  if (lhs.ndim < 2) throw new Error("lhs must have at least 2 dimensions");
  if (rhs.ndim < 2) throw new Error("rhs must have at least 2 dimensions");
  if (typeof padding === "string") {
    if (lhsDilation?.some((d) => d !== 1)) {
      throw new Error(
        "String padding is not supported for transposed convolutions",
      );
    }
    padding = padtypeToPads(
      lhs.shape.slice(2),
      rhs.shape.slice(2),
      windowStrides,
      rhsDilation ?? rep(rhs.ndim - 2, 1),
      padding,
    );
  }
  if (featureGroupCount !== 1) {
    // We implement grouped convolutions by using leading vmapDims in the
    // convolution operator, then concatenating at the end.
    //
    // lhs: [N, C_in, ...xs]         -> [G, N, C_in / G, ...xs]
    // rhs: [C_out, C_in / G, ...ks] -> [G, C_out / G, C_in / G, ...ks]
    //
    // Convolve normally to get [G, N, C_out / G, ...ys], then move the G axis
    // back and reshape to [N, C_out, ...ys].
    const G = featureGroupCount;
    const [N, C_in, ...xs] = lhs.shape;
    const [C_out, C_in_per_group, ...ks] = rhs.shape;
    if (C_in % G !== 0) {
      throw new Error(
        `featureGroupCount=${G} must divide input channels=${C_in}`,
      );
    }
    if (C_out % G !== 0) {
      throw new Error(
        `featureGroupCount=${G} must divide output channels=${C_out}`,
      );
    }
    if (C_in / G !== C_in_per_group) {
      throw new Error(
        `rhs input channels=${C_in_per_group} must equal lhs input channels / groups=${C_in / G}`,
      );
    }
    using lhsReshaped = lhs.reshape([N, G, C_in / G, ...xs]);
    using lhsGrouped = moveaxis(lhsReshaped, 1, 0);
    using rhsGrouped = rhs.reshape([G, C_out / G, C_in_per_group, ...ks]);
    using result = core.conv(lhsGrouped, rhsGrouped, {
      vmapDims: 1, // Batch over G
      strides: windowStrides,
      padding,
      lhsDilation,
      rhsDilation,
    }) as Array;
    const ys = result.shape.slice(3);
    using moved = moveaxis(result, 0, 1) as Array;
    return moved.reshape([N, C_out, ...ys]);
  }
  return core.conv(lhs, rhs, {
    strides: windowStrides,
    padding,
    lhsDilation,
    rhsDilation,
  }) as Array;
}

/** Convenience wrapper around `convGeneralDilated`. */
export function convWithGeneralPadding(
  lhs: Array,
  rhs: Array,
  windowStrides: number[],
  padding: PaddingType,
  lhsDilation?: number[],
  rhsDilation?: number[],
): Array {
  return convGeneralDilated(lhs, rhs, windowStrides, padding, {
    lhsDilation,
    rhsDilation,
  });
}

/** Convenience wrapper around `convGeneralDilated`. */
export function conv(
  lhs: Array,
  rhs: Array,
  windowStrides: number[],
  padding: PaddingType,
): Array {
  return convGeneralDilated(lhs, rhs, windowStrides, padding);
}

/**
 * Convenience wrapper for calculating the N-d convolution "transpose".
 *
 * This function directly calculates a fractionally strided conv rather than
 * indirectly calculating the gradient (transpose) of a forward convolution.
 * It is equivalent to the JAX version, except:
 *
 * - The `use_consistent_padding` option is not available. We only have the
 *   consistent padding case (JAX version >0.8.4).
 * - The order of dimensions matches `lax.conv_general_dilated`.
 *
 * Unlike PyTorch/TensorFlow, by default we don't reverse the kernel's spatial
 * dimensions or the `(C_out, C_in)` axis order. To get this behavior, set
 * `transposeKernel` to true.
 *
 * @param lhs - Input tensor; shape `[N, C_in, ...xs]`
 * @param rhs - Convolution kernel; shape `[C_out, C_in, ...ks]`
 * @param strides - Sequence of n integers, sets fractional stride
 * @param padding - Apply padding of `dilation * (kernel_size - 1) - padding` to
 *   each side of the input, so it acts like gradient of `conv()`
 * @param rhsDilation - Atrous dilation for the kernel
 * @param transposeKernel - Flip spatial axes and swap the input/output channels
 *   of the kernel; its shape should be `[C_in, C_out, ...ks]`
 */
export function convTranspose(
  lhs: Array,
  rhs: Array,
  strides: number[],
  padding: PaddingType,
  {
    rhsDilation,
    transposeKernel = false,
  }: {
    rhsDilation?: number[];
    transposeKernel?: boolean;
  } = {},
): Array {
  // Reference: https://github.com/jax-ml/jax/blob/c656803/jax/_src/lax/convolution.py#L296
  const kernelShape = rhs.shape.slice(2);
  // Calculate correct output shape from padding and strides.
  rhsDilation = rhsDilation ?? rep(kernelShape.length, 1);
  const effectiveKernel = kernelShape.map((k, i) =>
    Math.max(0, (k - 1) * rhsDilation[i] + 1),
  );
  const pads = effectiveKernel.map((k, i) =>
    convTransposePadding(
      k,
      strides[i],
      typeof padding === "string" ? padding : padding[i],
    ),
  );
  if (transposeKernel) {
    // Flip spatial axes and swap C_out/C_in.
    using flipped = core.flip(rhs, range(2, rhs.ndim)) as Array;
    rhs = moveaxis(flipped, 0, 1) as Array;
  }
  return convGeneralDilated(lhs, rhs, rep(lhs.ndim - 2, 1), pads, {
    lhsDilation: strides,
    rhsDilation,
  });
}

// Reference: https://github.com/jax-ml/jax/pull/32268
function convTransposePadding(
  k: number,
  s: number,
  padding: string | Pair,
): Pair {
  let padLen: number;
  let pad1: number;
  if (padding === "SAME") {
    padLen = k + s - 2;
    pad1 = s > k - 1 ? k - 1 : Math.ceil(padLen / 2);
  } else if (padding === "VALID") {
    padLen = k + s - 2 + Math.max(k - s, 0);
    pad1 = k - 1;
  } else if (JsArray.isArray(padding)) {
    const pads = [k - 1 - padding[0], k - 1 - padding[1]];
    pad1 = pads[0];
    padLen = pads[0] + pads[1];
  } else {
    throw new Error(`convTranspose: Invalid padding type ${padding}`);
  }
  return [pad1, padLen - pad1];
}

/** Reduce a computation over padded windows. */
export function reduceWindow(
  operand: Array,
  computation: (x: Array) => Array,
  windowDimensions: number[],
  windowStrides?: number[],
): Array {
  if (operand.ndim < windowDimensions.length) {
    throw new Error(
      `Operand dimensions ${operand.ndim} < window ${windowDimensions.length}`,
    );
  }
  if (!windowStrides) windowStrides = rep(windowDimensions.length, 1);

  for (let i = 0; i < operand.ndim; i++) {
    // Vmap the computation over any pre-pooled dimensions.
    computation = vmap(computation, 0) as any;
  }
  const pooled = bind1(Primitive.Pool, [operand], {
    window: windowDimensions,
    strides: windowStrides,
  }) as Array;
  const result = computation(pooled);
  if (pooled !== result) pooled[Symbol.dispose]?.();
  return result;
}

/** The error function: `erf(x) = 2/sqrt(pi) * int[0..x] exp(-t^2) dt`. */
export function erf(x: ArrayLike): Array {
  return core.erf(x) as Array;
}

/**
 * The complementary error function: `erfc(x) = 1 - erf(x)`.
 *
 * This function is more accurate than `1 - erf(x)` for large values of `x`,
 * where `erf(x)` is very close to 1.
 */
export function erfc(x: ArrayLike): Array {
  return core.erfc(x) as Array;
}

/**
 * Stops gradient computation.
 *
 * Behaves as the identity function but prevents the flow of gradients during
 * forward or reverse-mode automatic differentiation.
 */
export function stopGradient(x: ArrayLike): Array {
  return core.stopGradient(x) as Array;
}

/**
 * Returns top `k` values and their indices along the specified axis of operand.
 *
 * This is a _stable_ algorithm: If two elements are equal, the lower-index
 * element appears first.
 *
 * @returns A tuple of `(values, indices)`, where `values` and `indices` have
 * the same shape as `x`, except along `axis` where they have size `k`.
 */
export function topK(
  x: ArrayLike,
  k: number,
  axis: number = -1,
): [Array, Array] {
  x = fudgeArray(x);
  axis = checkAxis(axis, x.ndim);
  const size = x.shape[axis];

  if (k < 0 || k > size)
    throw new Error(`topK: k must be in the range [0, ${size}], got ${k}`);
  if (k === 0) {
    const outShape = x.shape.slice();
    outShape[axis] = 0;
    const y = zerosLike(x, { shape: outShape });
    const yi = zerosLike(x, { dtype: DType.Int32, shape: outShape });
    return [y, yi];
  }

  // We want to sort it in descending order, therefore we reverse before and
  // after argsort. This ensures that ties are resolved by smaller index.
  const flipped = core.flip(x, [axis]) as Array;
  const moved = moveaxis(flipped, axis, -1) as Array;
  const argsortResult = core.argsort(moved);
  const y = argsortResult[0];
  const yi = argsortResult[1];
  const extract = (a: core.Tracer) => {
    const sliced = a.slice(...rep(a.ndim - 1, [] as []), [-k]);
    const movedBack = moveaxis(sliced, -1, axis);
    const result = core.flip(movedBack, [axis]) as Array;
    if (movedBack !== sliced) (movedBack as Array).dispose();
    (sliced as Array).dispose();
    return result;
  };
  const neg = yi.neg() as Array;
  const adjusted = neg.add(size - 1) as Array;
  const result: [Array, Array] = [extract(y), extract(adjusted)];
  adjusted.dispose();
  neg.dispose();
  yi.dispose();
  y.dispose();
  if (moved !== flipped) (moved as Array).dispose();
  flipped.dispose();
  return result;
}
